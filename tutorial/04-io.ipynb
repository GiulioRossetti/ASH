{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de43be1a",
   "metadata": {},
   "source": [
    "<span>\n",
    "<img src=\"http://ash.readthedocs.io/en/latest/_static/ash.png\" width=\"260px\" align=\"right\"/>\n",
    "</span>\n",
    "<span>\n",
    "<b>Author:</b> <a href=\"https://andreafailla.github.io\">Andrea Failla</a><br/>\n",
    "<b>Python version:</b>  3.9<br/>\n",
    "<b>ASH version:</b>  0.1.0<br/>\n",
    "<b>Last update:</b> July 2025\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85944af",
   "metadata": {},
   "source": [
    "<a id=\"attributed-stream-hypergraphs-ash\"></a>\n",
    "# Attributed Stream Hypergraphs (ASH) - IO & Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d59b11b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -e ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82fa49",
   "metadata": {},
   "source": [
    "<a id=\"table-of-contents\"></a>\n",
    "# Table of Contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Creating an Example ASH](#creating-example-ash)\n",
    "- [Writing Node Profiles to CSV](#writing-profiles-csv)\n",
    "- [Reading Node Profiles from CSV](#reading-profiles-csv)\n",
    "- [Profiles JSONL (read/write + gzip)](#profiles-jsonl)\n",
    "- [Hyperedges CSV (structure only)](#hyperedges-csv)\n",
    "- [Full ASH JSON (read/write + gzip)](#ash-json)\n",
    "- [HIF Format (Hypergraph Interchange Format)](#hif-format)\n",
    "- [Integrity & Round-Trip Checks](#integrity-checks)\n",
    "- [Best Practices & Tips](#best-practices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70911afe",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "This tutorial covers persistence and interchange of ASH objects and their related data (node profiles and hyperedges).\n",
    "You will learn how to:\n",
    "\n",
    "1. Export/import time-varying node profiles (CSV & JSONL)\n",
    "2. Export/import timestamped hyperedges (CSV)\n",
    "3. Serialize/deserialize a complete ASH (JSON, optional gzip)\n",
    "4. Use the HIF (Hypergraph Interchange Format) exporter/importer for interoperability\n",
    "5. Perform integrity checks after round-trips\n",
    "\n",
    "[üîù To top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f975c",
   "metadata": {},
   "source": [
    "<a id=\"creating-example-ash\"></a>\n",
    "## Creating an Example ASH\n",
    "\n",
    "We'll build a small ASH with a few nodes, temporal node attributes, and hyperedges.\n",
    "This instance will be serialized using the different formats.\n",
    "\n",
    "[üîù To top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7261ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: [1, 2, 3]\n",
      "Hyperedges: ['e3', 'e1', 'e2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/var/folders/ym/_r1d94191y5bkvmtmz9tv7cw0000gn/T/ash_io_demo_ys24r7li'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ash_model import ASH, NProfile\n",
    "from ash_model.readwrite import (\n",
    "    write_profiles_to_csv, read_profiles_from_csv,\n",
    "    write_profiles_to_jsonl, read_profiles_from_jsonl,\n",
    "    write_sh_to_csv, read_sh_from_csv,\n",
    "    write_ash_to_json, read_ash_from_json,\n",
    "    write_hif, read_hif\n",
    ")\n",
    "import os, gzip, json, tempfile, shutil\n",
    "\n",
    "ash = ASH()\n",
    "# Node temporal attributes\n",
    "ash.add_node(1, start=0, end=2, attr_dict={\"role\": \"admin\", \"score\": 10})\n",
    "ash.add_node(1, start=3, attr_dict={\"role\": \"admin\", \"score\": 12})\n",
    "ash.add_node(2, start=0, end=3, attr_dict={\"role\": \"user\", \"score\": 5})\n",
    "ash.add_node(3, start=1, end=3, attr_dict={\"role\": \"guest\", \"score\": 7})\n",
    "\n",
    "# Hyperedges with attributes & temporal spans\n",
    "ash.add_hyperedge([1,2], start=0, end=1, weight=1.0, kind=\"pair\")\n",
    "ash.add_hyperedge([2,3], start=1, end=2, weight=2.0, kind=\"pair\")\n",
    "ash.add_hyperedge([1,2,3], start=2, end=3, weight=3.0, kind=\"group\")\n",
    "\n",
    "print(f\"Nodes: {list(ash.nodes())}\")\n",
    "print(f\"Hyperedges: {list(ash.hyperedges())}\")\n",
    "\n",
    "# Temp working directory for artifacts\n",
    "workdir = tempfile.mkdtemp(prefix=\"ash_io_demo_\")\n",
    "workdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88678df3",
   "metadata": {},
   "source": [
    "<a id=\"writing-profiles-csv\"></a>\n",
    "## Writing Node Profiles to CSV\n",
    "\n",
    "`write_profiles_to_csv` exports one row per (node, timestamp) with all node attributes.\n",
    "The header lists attribute names. Types are auto-inferred when reading.\n",
    "\n",
    "[üîù To top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc541b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_id,tid,role,score\n",
      "1,0,admin,10\n",
      "1,1,admin,10\n",
      "1,2,admin,10\n",
      "1,3,admin,12\n",
      "2,0,user,5\n",
      "2,1,user,5\n",
      "2,2,user,5\n",
      "2,3,user,5\n",
      "3,1,guest,7\n",
      "3,2,guest,7\n",
      "3,3,guest,7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profiles_csv = os.path.join(workdir, \"profiles.csv\")\n",
    "write_profiles_to_csv(ash, profiles_csv)\n",
    "print(open(profiles_csv).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d921a4",
   "metadata": {},
   "source": [
    "<a id=\"reading-profiles-csv\"></a>\n",
    "## Reading Node Profiles from CSV\n",
    "\n",
    "`read_profiles_from_csv` rebuilds a dict: `{node: {tid: NProfile}}` with type inference.\n",
    "\n",
    "[üîù To top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20989c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['node_id,tid,role,score\\n', '1,0,admin,10\\n', '1,1,admin,10\\n', '1,2,admin,10\\n', '1,3,admin,12\\n', '2,0,user,5\\n', '2,1,user,5\\n', '2,2,user,5\\n', '2,3,user,5\\n', '3,1,guest,7\\n', '3,2,guest,7\\n', '3,3,guest,7\\n']\n",
      "0 {'role': 'admin', 'score': 10}\n",
      "1 {'role': 'admin', 'score': 10}\n",
      "2 {'role': 'admin', 'score': 10}\n",
      "3 {'role': 'admin', 'score': 12}\n"
     ]
    }
   ],
   "source": [
    "profiles_dict = read_profiles_from_csv(profiles_csv)\n",
    "# Show reconstructed profile for node 1 over time\n",
    "for tid, prof in profiles_dict[1].items():\n",
    "    print(tid, prof.get_attributes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e11a03",
   "metadata": {},
   "source": [
    "<a id=\"profiles-jsonl\"></a>\n",
    "## Profiles JSONL (read/write + gzip)\n",
    "\n",
    "`write_profiles_to_jsonl` appends one JSON object per (node, tid). Optional gzip compression.\n",
    "Recommended for large datasets due to streaming-friendly format.\n",
    "\n",
    "[üîù To top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0d3cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample lines:\n",
      "{\"node_id\": 1, \"attrs\": {\"role\": \"admin\", \"score\": 10}, \"tid\": 0}\n",
      "{\"node_id\": 1, \"attrs\": {\"role\": \"admin\", \"score\": 10}, \"tid\": 1}\n",
      "{\"node_id\": 1, \"attrs\": {\"role\": \"admin\", \"score\": 10}, \"tid\": 2}\n",
      "Compressed first line: {\"node_id\": 1, \"attrs\": {\"role\": \"admin\", \"score\": 10}, \"tid\": 0}\n",
      "Node 2 tids (plain): [0, 1, 2, 3]\n",
      "Node 2 tids (gzip): [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "profiles_jsonl = os.path.join(workdir, \"profiles.jsonl\")\n",
    "write_profiles_to_jsonl(ash, profiles_jsonl)\n",
    "print(\"Sample lines:\")\n",
    "print('\\n'.join(open(profiles_jsonl).read().splitlines()[:3]))\n",
    "\n",
    "# Gzip compressed\n",
    "profiles_jsonl_gz = profiles_jsonl + \".gz\"\n",
    "write_profiles_to_jsonl(ash, profiles_jsonl_gz, compress=True)\n",
    "with gzip.open(profiles_jsonl_gz, 'rt') as f:\n",
    "    print(\"Compressed first line:\", f.readline().strip())\n",
    "\n",
    "# Read back\n",
    "jsonl_profiles = read_profiles_from_jsonl(profiles_jsonl)\n",
    "jsonl_profiles_gz = read_profiles_from_jsonl(profiles_jsonl_gz, compress=True)\n",
    "print(\"Node 2 tids (plain):\", list(jsonl_profiles[2].keys()))\n",
    "print(\"Node 2 tids (gzip):\", list(jsonl_profiles_gz[2].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4642bc64",
   "metadata": {},
   "source": [
    "<a id=\"hyperedges-csv\"></a>\n",
    "## Hyperedges CSV (structure only)\n",
    "\n",
    "`write_sh_to_csv` exports each hyperedge presence interval as a row: `n1,n2,...\\tstart,end`.\n",
    "Attributes are NOT preserved. Use for lightweight temporal structure dumps.\n",
    "\n",
    "[üîù To top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8450996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes\tstart,end\n",
      "1,2,3\t2,3\n",
      "1,2\t0,1\n",
      "2,3\t1,2\n",
      "\n",
      "Recovered hyperedges count: 3\n"
     ]
    }
   ],
   "source": [
    "hedges_csv = os.path.join(workdir, \"hyperedges.csv\")\n",
    "write_sh_to_csv(ash, hedges_csv)\n",
    "print(open(hedges_csv).read())\n",
    "\n",
    "ash_from_hedges = read_sh_from_csv(hedges_csv)\n",
    "print(\"Recovered hyperedges count:\", ash_from_hedges.number_of_hyperedges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f8afc5",
   "metadata": {},
   "source": [
    "<a id=\"ash-json\"></a>\n",
    "## Full ASH JSON (read/write + gzip)\n",
    "\n",
    "`write_ash_to_json` serializes the entire structure including attributes.\n",
    "`read_ash_from_json` reconstructs it (re-adding hyperedges over time).\n",
    "Use gzip for large networks.\n",
    "\n",
    "[üîù To top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f418d782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON size (bytes): 1513\n",
      "Gzip JSON size (bytes): 280\n",
      "Loaded nodes: 3 Loaded hyperedges: 3\n"
     ]
    }
   ],
   "source": [
    "ash_json = os.path.join(workdir, \"ash.json\")\n",
    "write_ash_to_json(ash, ash_json)\n",
    "print(\"JSON size (bytes):\", os.path.getsize(ash_json))\n",
    "\n",
    "ash_json_gz = ash_json + \".gz\"\n",
    "write_ash_to_json(ash, ash_json_gz, compress=True)\n",
    "print(\"Gzip JSON size (bytes):\", os.path.getsize(ash_json_gz))\n",
    "\n",
    "ash_loaded = read_ash_from_json(ash_json)\n",
    "print(\"Loaded nodes:\", ash_loaded.number_of_nodes(), \"Loaded hyperedges:\", ash_loaded.number_of_hyperedges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97048512",
   "metadata": {},
   "source": [
    "<a id=\"hif-format\"></a>\n",
    "## HIF Format (Hypergraph Interchange Format)\n",
    "\n",
    "`write_hif` exports a structured JSON containing:\n",
    "- `incidences`: one record per node‚Äìhyperedge membership (with optional weight)\n",
    "- `nodes`: each node with time-varying attributes collapsed into (start,end,value) spans plus `_presence`\n",
    "- `edges`: hyperedge attributes plus `_presence` intervals\n",
    "\n",
    "This enables interoperability while preserving temporal semantics compactly.\n",
    "\n",
    "[üîù To top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df0a80f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIF excerpt:\n",
      "{\n",
      "  \"network-type\": \"undirected\",\n",
      "  \"metadata\": {\n",
      "    \"dataset\": \"demo\",\n",
      "    \"version\": 1\n",
      "  },\n",
      "  \"incidences\": [\n",
      "    {\n",
      "      \"node\": 1,\n",
      "      \"edge\": \"e3\",\n",
      "      \"weight\": 3.0\n",
      "    },\n",
      "    {\n",
      "      \"node\": 2,\n",
      "      \"edge\": \"e3\",\n",
      "      \"weight\": 3.0\n",
      "    },\n",
      "    {\n",
      "      \"node\": 3,\n",
      "      \"edge\": \"e3\",\n",
      "      \"weight\": 3.0\n",
      "    },\n",
      "    {\n",
      "      \"node\": 1,\n",
      "      \"edge\": \"e1\"\n",
      "    },\n",
      "    {\n",
      "      \"node\": 2,\n",
      "      \"edge\": \"e1\"\n",
      "    },\n",
      "    {\n",
      "      \"node\": 2,\n",
      "      \"edge\": \"e2\",\n",
      "      \"weight\": 2.0\n",
      "    },\n",
      "    {\n",
      "      \"node\": 3,\n",
      "      \"edge\": \"e2\",\n",
      "      \"weight\": 2.0\n",
      "    }\n",
      "  ],\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"node\": 1,\n",
      "      \"attrs\": {\n",
      "        \"role\": [\n",
      "          [\n",
      "            0,\n",
      "            3,\n",
      "            \"admin\"\n",
      "          ]\n",
      "        ],\n",
      "        \"score\": [\n",
      "          [\n",
      "            0,\n",
      "            2,\n",
      "            10\n",
      "          ],\n",
      "          [\n",
      "            3,\n",
      "            3,\n",
      "            12\n",
      "          ]\n",
      "        ],\n",
      "        \"_presence\": [\n",
      "          [\n",
      "            0,\n",
      "            3\n",
      "          ]\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"node\": 2,\n",
      "      \"attrs\": {\n",
      "        \"role\": [\n",
      "          [\n",
      "            0,\n",
      "            3,\n",
      "            \"user\"\n",
      "          ]\n",
      "        ],\n",
      "        \"score\": [\n",
      "          [\n",
      "            0,\n",
      "            3,\n",
      "            5\n",
      "          ]\n",
      "        ],\n",
      "        \"_presence\": [\n",
      "          [\n",
      "            0,\n",
      "            3\n",
      "          ]\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"node\": 3,\n",
      "      \"attrs\": {\n",
      "        \"role\": [\n",
      "          [\n",
      "            1,\n",
      "            3,\n",
      "            \"guest\"\n",
      "          ]\n",
      "        ],\n",
      "        \"score\": [\n",
      "          [\n",
      "            1,\n",
      "            3,\n",
      "            7\n",
      "          ]\n",
      "        ],\n",
      "        \"_presence\": [\n",
      "          [\n",
      "            1,\n",
      "            3\n",
      "          ]\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"edges\": [\n",
      "    {\n",
      "      \"edge\": \"e3\",\n",
      "      \"attrs\": {\n",
      "        \"weight\": 3.0,\n",
      "        \"kind\": \"group\",\n",
      "        \"_presence\": [\n",
      "          [\n",
      "            2,\n",
      "            3\n",
      "          ]\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"edge\": \"e1\",\n",
      "      \"attrs\": {\n",
      "        \"weight\": 1.0,\n",
      "        \"kind\": \"pair\",\n",
      "        \"_presence\": [\n",
      "          [\n",
      "            0,\n",
      "            1\n",
      "          ]\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"edge\": \"e2\",\n",
      "      \"attrs\": {\n",
      "        \"weight\": 2.0,\n",
      "        \"kind\": \"pair\",\n",
      "        \"_presence\": [\n",
      "          [\n",
      "            1,\n",
      "            2\n",
      "          ]\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "HIF loaded nodes: 3 hyperedges: 3\n"
     ]
    }
   ],
   "source": [
    "hif_path = os.path.join(workdir, \"ash.hif.json\")\n",
    "write_hif(ash, hif_path, metadata={\"dataset\": \"demo\", \"version\": 1})\n",
    "print(\"HIF excerpt:\")\n",
    "print('\\n'.join(open(hif_path).read().splitlines()))\n",
    "\n",
    "ash_hif_loaded = read_hif(hif_path)\n",
    "print(\"HIF loaded nodes:\", ash_hif_loaded.number_of_nodes(), \"hyperedges:\", ash_hif_loaded.number_of_hyperedges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452f76f",
   "metadata": {},
   "source": [
    "<a id=\"integrity-checks\"></a>\n",
    "## Integrity & Round-Trip Checks\n",
    "\n",
    "We compare basic statistics (counts, attribute sets) after reloading from different formats.\n",
    "For deeper checks you could diff presence intervals & attribute timelines.\n",
    "\n",
    "[üîù To top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b46877e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: {'nodes': 3, 'hyperedges': 3, 'node_attrs': ['role', 'score'], 'hedge_attrs': ['kind', 'weight']}\n",
      "From hedges CSV (no attrs): {'nodes': 3, 'hyperedges': 3, 'node_attrs': [], 'hedge_attrs': ['weight']}\n",
      "From JSON: {'nodes': 3, 'hyperedges': 3, 'node_attrs': ['role', 'score'], 'hedge_attrs': ['kind', 'weight']}\n",
      "From HIF: {'nodes': 3, 'hyperedges': 3, 'node_attrs': ['role', 'score'], 'hedge_attrs': ['kind', 'weight']}\n"
     ]
    }
   ],
   "source": [
    "def summary(h):\n",
    "    return {\n",
    "        'nodes': h.number_of_nodes(),\n",
    "        'hyperedges': h.number_of_hyperedges(),\n",
    "        'node_attrs': sorted(h.list_node_attributes().keys()),\n",
    "        'hedge_attrs': sorted(h.list_hyperedge_attributes().keys())\n",
    "    }\n",
    "\n",
    "print('Original:', summary(ash))\n",
    "print('From hedges CSV (no attrs):', summary(ash_from_hedges))\n",
    "print('From JSON:', summary(ash_loaded))\n",
    "print('From HIF:', summary(ash_hif_loaded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e3bea",
   "metadata": {},
   "source": [
    "<a id=\"best-practices\"></a>\n",
    "## Best Practices & Tips\n",
    "\n",
    "- Use JSONL (optionally gzipped) for very large node profile timelines (streaming friendly).\n",
    "- Use full JSON (gzip) when you need a faithful snapshot including attributes.\n",
    "- Use HIF for interoperability / archival: compact temporal attribute encoding.\n",
    "- Hyperedges CSV is lossy (drops attributes) ‚Äî only for quick topology experiments.\n",
    "- Always validate round-trip with basic stats (& optionally invariants like degree distribution).\n",
    "- Keep metadata (e.g. preprocessing parameters) inside HIF `metadata`.\n",
    "\n",
    "[üîù To top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cca95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: cleanup temporary directory when done\n",
    "shutil.rmtree(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc314359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
